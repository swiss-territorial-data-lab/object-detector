# Produce tile geometries based on the AoI extent and zoom level
prepare_data.py:  
  srs: EPSG:2056
  datasets:
    shapefile: ./data/labels/tlm-hr-trn-topo.shp                   # GT labels
    # fp_shapefile: ./data/FP/<SHPFILE>                           # FP labels
    # empty_tiles_aoi: ./data/AoI/<SHPFILE>                      # AOI in which additional empty tiles can be selected. Only one 'empty_tiles' option can be selected  
    # empty_tiles_year: 2020                                       # If "empty_tiles_aoi" selected then provide a year. Choice: (1) numeric (i.e. 2020), (2) [year1, year2] (random selection of a year within a given year range) 
    # empty_tiles_shp: .data/empty_tiles/<SHPFILE>                 # Provided shapefile of selected empty tiles. Only one 'empty_tiles' option can be selected  
  output_folder: ./output/trne/
  zoom_level: 16

# Fetch of tiles and split into 3 datasets: train, test, validation
generate_tilesets.py:
  debug_mode: 
    enable: False  # sample of tiles
    nb_tiles_max: 5000
  working_directory: output
  datasets:
    aoi_tiles: trne/tiles.geojson
    ground_truth_labels: trne/labels.geojson
    # add_fp_labels:
    #   fp_labels: trne/FP.geojson 
    #   frac_trn: 0.7        # fraction of fp tiles to add to the trn dataset, then the remaining tiles will be split in 2 and added to tst and val datasets                          
    image_source:
      type: XYZ                             # supported values: 1. MIL = Map Image Layer 2. WMS 3. XYZ 4. FOLDER
      year: 2020                   # supported values: 1. multi-year (tiles of different year), 2. <year> (i.e. 2020)
      location: https://wmts.geo.admin.ch/1.0.0/ch.swisstopo.swissimage-product/default/{year}/3857/{z}/{x}/{y}.jpeg
  # add_empty_tiles:            # add empty tiles to datasets
  #   tiles_frac: 0.5       # fraction (relative to the number of tiles intersecting labels) of empty tiles to add
  #   frac_trn: 0.7         # fraction of empty tiles to add to the trn dataset, then the remaining tiles will be split in 2 and added to tst and val datasets
  #   keep_oth_tiles: False # keep tiles in oth dataset not intersecting oth labels
  output_folder: trne/
  tile_size: 256          # per side, in pixels
  seed: 42
  overwrite: True
  n_jobs: 10
  COCO_metadata:
    year: 2021
    version: 1.0
    description: Swiss Image Hinterground w/ Quarries and Mineral Exploitation Sites detection
    contributor: swisstopo
    url: https://swisstopo.ch
    license:
      name: unknown
      url: unknown

# Train the model with the detectron2 algorithm
train_model.py:
  working_directory: ./output/trne/
  log_subfolder: logs
  sample_tagged_img_subfolder: sample_tagged_images
  COCO_files: # relative paths, w/ respect to the working_folder
    trn: COCO_trn.json
    val: COCO_val.json
    tst: COCO_tst.json
  detectron2_config_file: ../../detectron2_config.yaml # path relative to the working_folder
  model_weights:
    model_zoo_checkpoint_url: COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml

# Object detection with the optimised trained model
make_detections.py:
  working_directory: ./output/trne/
  log_subfolder: logs
  sample_tagged_img_subfolder: sample_tagged_images
  COCO_files:           # relative paths, w/ respect to the working_folder
    trn: COCO_trn.json
    val: COCO_val.json
    tst: COCO_tst.json
  detectron2_config_file: ../../detectron2_config.yaml # path relative to the working_folder
  model_weights:
    pth_file: ./logs/model_0002999.pth # trained model minimising the validation loss curve, monitor the training process via tensorboard (tensorboard --logdir </logs>)
  image_metadata_json: img_metadata.json
  rdp_simplification:   # rdp = Ramer-Douglas-Peucker
    enabled: True
    epsilon: 2.0        # cf. https://rdp.readthedocs.io/en/latest/
  score_lower_threshold: 0.05
  remove_det_overlap: False  # if several detections overlap (IoU > 0.5), only the one with the highest confidence score is retained. Not recommended for use with a single class.
    
# Evaluate the quality of the detections for the different datasets by calculating metrics
assess_detections.py:
  working_directory: ./output/trne/
  datasets:
    ground_truth_labels: labels.geojson
    image_metadata_json: img_metadata.json
    split_aoi_tiles: split_aoi_tiles.geojson # aoi = Area of Interest
    categories: category_ids.json
    detections:
      trn: trn_detections_at_0dot05_threshold.gpkg
      val: val_detections_at_0dot05_threshold.gpkg
      tst: tst_detections_at_0dot05_threshold.gpkg
  output_folder: .
  iou_threshold: 0.1
  area_threshold: 50       # area under which the polygons are discarded from assessment
  metrics_method: macro-average   # 1: macro-average ; 3: macro-weighted-average ; 2: micro-average
